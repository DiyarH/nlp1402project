\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% ready for submission
% \usepackage{neurips_2019}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{neurips_2019}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
% \usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage[a4paper,
textwidth=16cm,
textheight=24cm,
headheight=1cm,
centering]{geometry}
\usepackage{titling}
\usepackage{fancyhdr}
\usepackage{caption}
\usepackage{csvsimple}
\usepackage{color}
\usepackage{pgffor}
\usepackage{tikz}
\usepackage{tikzpagenodes}
\usepackage{subfig}
\usepackage{xparse}

\pretitle{
  \begin{center}
    % References here
  \LARGE
  \includegraphics[width=4cm]{latex/iust_logo.png}\\[\bigskipamount]
}
\posttitle{
  \end{center}
}
\title{Natural Language Processing Project Phase 2 Report}
\author{
  Diyar Hamedi\\
  Department of Computer Engineering\\
  Iran University of Science and Technology\\
  \texttt{diyar\_hamedi@comp.iust.ac.ir}\\
}
\date{Summer 2023}

\begin{document}

\pagestyle{fancy}
\fancyhead{}
\fancyhead[L]{
  \begin{minipage}{0.1\textwidth}
    \includegraphics[width=1cm]{latex/iust_logo.png}
  \end{minipage}
  \begin{minipage}{0.5\textwidth}\raggedright
    Natural Language Processing Phase 2 Report\\
    Iran University of Science and Technology\\
  \end{minipage}
}
\setlength{\headheight}{35.11143pt}

\maketitle

\begin{abstract}
  This report presents the methods employed in the second phase of developing a text topic classification model.
  This includes analyzing the dataset gathered in the previous phase, extracting different features and designing and training an effective architecture.
  We will also explain the insights gained from the analysis and showcase the results of the experiments made with different features and architectures.
  The code for reproducing this file, the results of the experiments and training the models is available at this repository: \url{https://github.com/DiyarH/nlp1402project}
\end{abstract}

\section{Introduction}
After gathering a dataset for text topic classification from the HowTo100M dataset\footnote{\url{https://www.di.ens.fr/willow/research/howto100m/}},
cleaning up the data and extracting different metrics and statistics from the data, we continue with the process of developing a model suitable for such task.
In order to find the best architecture for the model and the features given to it as input, we perform several analysis and experiments.
First, we make use of the Word2Vec algorithm to generate a brand new embedding for all of the dataset words and examine patterns and biases which appear in the embedding.
Next, we finetune a language model on each of the categorized datasets, and compare the generated sentences of the said model to the actual data.
In the following steps, we extract various features from the data and train simple architectures on each of those features separately to gain insight on their usefulness.
Using the knowledge acquired, we then develop multiple architectures which take a combination of said features as input, and compare their training performance as well.
Finally, we explore the usefulness of Large Language Models (LLMs) for research in Natural Language Processing (NLP), particularly for artificial data generation and zero-shot tasks.

\section{Experiments}
In this section, we give a brief explanation for each of the steps involved in the experiments made on the dataset and different NLP methodologies.

\subsection{Word2Vec}
Using the classic Word2Vec algorithm, we generate an original matrix of word embeddings for each of the categorized datasets from scratch.
% After acquiring the word vectors, we perform several analysis on them in order to discover emerging word relationships based on the similarity and distance between the words and patterns that follow,
% and reveal and discuss the biases that are present in the dataset.
We also train a word embedding using Word2Vec on the entire dataset, which is used in the following experiments.

\subsection{Language Model}
In this section, we select an auto-regressive Language Model (LM) and finetune it on each of the categorized datasets separately.
For this experiment we choose GPT2, due to its lightweightness and its availability on HuggingFace.
This allows us to leverage the HuggingFace Trainer API, which trivializes the training process.
Here, we present some of the samples generated by the finetuned GPT2 for a few of the categories after finetuning for 1 epoch:

\begin{itemize}
  \item Home and Garden: с la, bien, o. The way. They say to be aware, what they really do was take care of this place, they do, very important things at this junkyard so everybody here you might like what I've had before I've left you that there are a large few boxes...
  \item Cars \& Other Vehicles: ia davids orok d1 ikke ww wakw oezyz yaw. What it is with your guys in here is a small motor bike repair team at home who has an 80-year, ronda c/c hodge r4 motor to repair cars or dicierals are you up against ia d1 of your...
  \item Personal Care and Style: ི my eyes, or your forehead has, it been infected with the plague on it. The blood I got that there a person has a infection that looks pretty similar but this doesn't exist, as I think about doing so, the rest  and they didn't show, maybe this isn't bad though it doesn't, do really take anything and if people really can do better than this...
  \item Travel: for the word, you did one hundred ten of, twelve thousand eighty eight three ten of thirteen, twelve. twelve thousand number to be used to put up that word for. 12 words because you, it might be different from one thousand...
  \item Holidays and Traditions: 【camel of your country's ˚◈; I like ********/ˏ/ˀ; And if not, give me to please, yes that was fun that evening when our first lady ˈ I POLICE, please get that * the  a, * ********.**************** for that one? What's. It for?   here at hallu is an image and we also like seeing some amazing examples to begin with and keep the ballad here...
  \item Youth: ick is to give the people something nice it feels nice ixi'm to give her it as usual a thumbs up after some tea at first incense I'll never have enough for another night's entertainment of today is you really need tea tonight tonight? Why yes sir...
  \item Hobbies and Crafts: with I know and how good it is, oh god thank God uploading. And to make sure I stay away and watch these, and you still to. The morning is a very kind, kind,,,, yours know in person, right about. All a long show from when they's, gonna. All this is, about this evening...
  \item ips. They're a fun for us   to listen as we try each new chord. My main thing and some more fun bits of all. third, and four. When they do music for their next musical band:
  
  So when it hits our fourth birthday dance party I will be my dance to another dance for you're first year to make sure there won't get my song...
\end{itemize}

It is apperant that while the models each use a few of the words specific to the category they are finetuned on, the samples generated are very different from normal subtitle texts.
One possible way to improve the quality of the generated samples is to simply finetune the models for more epochs, as one epoch of finefuning does not seem to be enough.

\subsection{Feature Engineering}
The purpose of this experiment is to compare different features extracted from each sample and then decide on the features selected as input for training the future models.
Each of the following features are extracted from each sample of the dataset and then used in isolation as input to the appropriate architecture:

\begin{itemize}
  \item Sentence length: The length of the sample in words is used as the only feature.
    A basic Multi-Layer Perceptron (MLP) model is trained using this feature.
  \item Word lengths: The lengths of each word in the sample is used as the feature.
  \item Words: The indices of each word in the sample is used as the feature.
  \item Word bigrams: This indices of each word in the sample and its previous word are concatenated as the feature.
  \item Word2Vec: The Word2Vec matrix trained in the first section is used to get the vector corresponding to each word in the sample as the feature.
  \item Word2Vec bigrams: The Word2Vec vectors for each word in the sample and its previous word are concatenated as the feature.
  \item BERT: Using the embedding weights of the BERT model (specifically the 'bert-base-uncased' model from HuggingFace), the vectors corresponding to each word in the sample are used as the feature.
\end{itemize}

The architecture of the model used in this experiment for the features other than the sentence length is made of a multilayer, bidirectional LSTM followed by a linear layer to map the LSTM outputs to class predictions.

\subsection{Classification using OpenAI}
In the last section of this report, we propose a simple zero-shot prompt to be given to LLMs such as ChatGPT, GPT4 and LLaMA for text topic classification.
The prompt is as follows:

\begin{quote}
  \input{src/openai_classification/prompt.txt}
\end{quote}

\end{document}